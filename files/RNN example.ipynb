{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd99c470",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b38b8",
   "metadata": {},
   "source": [
    "## What is an RNN?\n",
    "A Recurrent Neural Network (RNN) is a type of neural network specifically designed to handle sequential data, like text, speech, or time-series data. \\\n",
    "Unlike MLPs, RNNs have `Memory` which allows it to use information from past inputs to influence the current output. \\\n",
    "An MLP is memoryless. It processes each input independently, making it unsuitable for tasks where context and order are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc94c33c",
   "metadata": {},
   "source": [
    "RNNs have three key elements:\n",
    "1. The Input: This is the current item in the sequence.\n",
    "2. The Hidden State: This is the memory of the network. It is a vector that holds information about previous steps in the sequence.\n",
    "3. The Output: This is the prediction at the current step.\n",
    "\n",
    "Throughout all of this, the same three weight matrices are used ($W_{xh}, W_{hh}, \\text{and }W_{hy}$).\n",
    "## Breaking Down the Weight Matrices\n",
    "\n",
    "Let's break down each of the weight matrices you see in an RNN diagram:\n",
    "\n",
    "* **$W_{xh}$ (Input-to-Hidden):** This matrix is used to process the current input ($x_t$). It transforms the input data into a vector that can be combined with the hidden state. Think of it as the \"new information\" gate.\n",
    "<br>\n",
    "* **$W_{hh}$ (Hidden-to-Hidden):** This matrix is used to process the previous hidden state ($h_{t-1}$). It transforms the \"memory\" from the last step so it can be combined with the current input. This is the \"old memory\" gate.\n",
    "<br>\n",
    "* **$W_{yh}$ (Hidden-to-Output):** This matrix takes the current hidden state ($h_t$) and produces the final output ($y_t$) for that step. It's the part of the network that makes a prediction.\n",
    "\n",
    "***\n",
    "\n",
    "## How They Are Used to Predict\n",
    "\n",
    "At each step in the sequence, the RNN calculates a new hidden state and a new output. The process looks like this:\n",
    "\n",
    "1.  The current input ($x_t$) is multiplied by the **$W_{xh}$** matrix.\n",
    "2.  The previous hidden state ($h_{t-1}$) is multiplied by the **$W_{hh}$** matrix.\n",
    "3.  The results of these two operations are summed, and then a non-linear activation function (like `tanh` or `ReLU`) is applied. This produces the new hidden state, $h_t$.\n",
    "4.  The new hidden state ($h_t$) is then multiplied by the **$W_{yh}$** matrix to produce the output for that step, $y_t$.\n",
    "\n",
    "The magic is that the *same* **$W_{xh}$**, **$W_{hh}$**, and **$W_{yh}$** matrices are used for every single step of the sequence, which is what allows the RNN to learn patterns in sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80805bd7",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time (BPTT)\n",
    "\n",
    "RNNs use a specific form of backpropagation called **Backpropagation Through Time (BPTT)**. The core idea is the same as standard backpropagation, but adapted for sequences:\n",
    "\n",
    "1.  The network makes a forward pass through the *entire sequence*, calculating the hidden states and outputs for each step.\n",
    "2.  The loss is calculated at the end of the sequence (or at each step, depending on the task).\n",
    "3.  The gradients are then propagated backward through the network, but also **backward through time**, from the final step of the sequence to the first.\n",
    "4.  The optimizer uses these accumulated gradients to update the shared weight matrices (**$W_{xh}$**, **$W_{hh}$**, and **$W_{yh}$**).\n",
    "\n",
    "***\n",
    "\n",
    "The key thing to remember is that because the **same weights** are used at every time step, the gradients from all steps in the sequence are **summed up** before the optimizer updates the weights. This is what allows the network to learn how its parameters affect the output across the entire sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da392dd",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "A great public dataset for this is the IMDB movie review dataset, which contains thousands of movie reviews labeled as either positive or negative. We can use our RNN to classify the sentiment of a review. \\\n",
    "However, since this is real data, we'll need to do some more robust preprocessing. We'll need to:\n",
    "\n",
    "1. Tokenize the reviews into individual words.\n",
    "\n",
    "2. Create a vocabulary of all unique words.\n",
    "\n",
    "3. Handle sentences of different lengths (since reviews aren't all the same length).\n",
    "\n",
    "4. Convert words into embeddings to feed into our RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11492716",
   "metadata": {},
   "source": [
    "**Disclaimer**: In a real-world scenario with a large text dataset, you would perform extensive preprocessing. This would include creating a vocabulary and using word embeddings to represent your words as vectors. For this simplified example, we'll use a dummy tensor to represent our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bde4aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # The RNN layer itself\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # The final linear layer to output the prediction\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The RNN layer returns both the output and the hidden state\n",
    "        rnn_out, hidden_state = self.rnn(x)\n",
    "        \n",
    "        # We'll use the output of the last step to make our final prediction\n",
    "        prediction = self.fc(rnn_out[:, -1, :])\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd81278",
   "metadata": {},
   "source": [
    "## \"Data\" Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4032a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll have a batch of 2 \"sentences\" (sequences of word vectors)\n",
    "# Each sentence has a sequence length of 5 and a word vector size of 10\n",
    "input_data = torch.randn(2, 5, 10) \n",
    "labels = torch.tensor([1, 0]).float().unsqueeze(1) # 1 for positive, 0 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11792e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.7645\n",
      "Epoch [2/5], Loss: 0.7104\n",
      "Epoch [3/5], Loss: 0.6583\n",
      "Epoch [4/5], Loss: 0.6083\n",
      "Epoch [5/5], Loss: 0.5605\n"
     ]
    }
   ],
   "source": [
    "## Training Loop\n",
    "model = SimpleRNN(input_size=10, hidden_size=20, output_size=1)\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. The Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(input_data)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79d49f",
   "metadata": {},
   "source": [
    "# Limitations of RNNs\n",
    "The two main limitations of RNNs are the **Vanishing Gradient Problem** and the **Exploding Gradient Problem**.\n",
    "1. **Vanishing Gradient Problem**: In a simple RNN, gradients are used to update the network's weights during backpropagation. As these gradients are propagated backward through time, they can become smaller and smaller with each step. This makes the gradient in earlier layer become close to zero. \n",
    "2. **Exploding Gradient Problem**: This is the opposite of the vanishing gradient problem. It occurs when the gradients become extremely large and unstable as they are propagated backward through time. \n",
    "\n",
    "## Example of the Vanishing Gradient Problem in RNNs\n",
    "Imagine a very simple, single-neuron RNN with a single recurrent weight, **$W_{hh} = 0.5$**. The gradient of the loss with respect to this weight is calculated by repeatedly multiplying the error from the output back through the network's layers and through time.\n",
    "\n",
    "For our simple case, let's say the gradient at the final step (t=5) is `1.0`.\n",
    "\n",
    "***\n",
    "\n",
    "**Step 5:** The gradient is `1.0`. To get the gradient for step 4, we multiply by the weight:\n",
    "* Gradient at t=4: $1.0 \\times W_{hh} = 1.0 \\times 0.5 = 0.5$\n",
    "\n",
    "**Step 4:** The gradient is `0.5`. To get the gradient for step 3, we multiply by the weight again:\n",
    "* Gradient at t=3: $0.5 \\times W_{hh} = 0.5 \\times 0.5 = 0.25$\n",
    "\n",
    "**Step 3:** The gradient is `0.25`. To get the gradient for step 2, we multiply by the weight again:\n",
    "* Gradient at t=2: $0.25 \\times W_{hh} = 0.25 \\times 0.5 = 0.125$\n",
    "\n",
    "**Step 2:** The gradient is `0.125`. To get the gradient for step 1, we multiply by the weight again:\n",
    "* Gradient at t=1: $0.125 \\times W_{hh} = 0.125 \\times 0.5 = 0.0625$\n",
    "\n",
    "***\n",
    "\n",
    "As you can see, the gradient quickly shrinks as it moves backward in time, going from `1.0` all the way down to `0.0625` in just a few steps.\n",
    "\n",
    "This is a simplified example, but it illustrates the core issue. In a real RNN, the gradient is multiplied by a series of terms, some of which are very small. When these small numbers are multiplied together repeatedly over a long sequence, the final gradient at the beginning of the sequence becomes so small that it is **effectively zero**, and the early layers of the network cannot be updated. This is why the model \"forgets\" information from earlier in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4d1a1",
   "metadata": {},
   "source": [
    "Architectures like LSTMs and GRUs were specifically designed to solve these two problems. They have a more complex internal structure that allows them to better control the flow of gradients and information, giving them a much more robust \"memory.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
