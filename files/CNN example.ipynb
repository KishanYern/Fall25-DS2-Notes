{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6aa61759",
      "metadata": {
        "id": "6aa61759"
      },
      "source": [
        "# Convolutional Neural Networks (CNNs)\n",
        "## What is a CNN?\n",
        "A convolutional neural network (CNN) rethinks the fully connected layers of an MLP to make them smarter and more efficient for grid-like data, such as images.\n",
        "\n",
        "You can think of the convolutional layer as an MLP that:\n",
        "1. Doesn't flatten the input, preserving its spatial structure.\n",
        "2. Replaces a fully connected layer with a convolutional layer, which is just a \"smarter\" layer that enforces local connections and reuses the same weights (the kernel) across the entire image.\n",
        "\n",
        "## What is a Kernal?\n",
        "A kernel (also called a filter) in a Convolutional Neural Network (CNN) is a small matrix of learnable weights that acts as a specialized feature detector.\n",
        "The CNN's job is to slide this pattern over every part of an input image to see where the pattern matches. The result is a new image, called a feature map, that highlights where the specific feature was found.\n",
        "\n",
        "## What is a Pooling Layer?\n",
        "A pooling layer in a CNN performs downsampling, systematically reducing the spatial size of a feature map. Its main job is to summarize the features present in a region of the feature map.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33be48ad",
      "metadata": {
        "id": "33be48ad"
      },
      "source": [
        "# CNN Architecture\n",
        "The full network can be divided into two main parts:\n",
        "### Feature Extraction Base\n",
        "This is the part of the network that houses the convolutional and pooling layers. Its job is to automatically learn and extract meaningful features from the input images. Between the convolutional and pooling layers, there is also an activation layer (ex: ReLU). This introduces non-linearity, allowing the model to learn more complex patterns.\n",
        "\n",
        "### Classifier Head\n",
        "This is the part of the network that takes the high level features learnt by the feature extraction base and uses them to make a final prediction. This part contains the Flatten Layer, which converts the 2D shape into a single 1D vector, Fully Connected Layers, and an Output Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e85ebb04",
      "metadata": {
        "id": "e85ebb04"
      },
      "source": [
        "# Implementation of a CNN\n",
        "In this example, we will be using the CIFAR-10 dataset. This dataset contains 60000 32x32 colored images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 testing images. Information on the dataset can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html) \\\n",
        "\\\n",
        "NOTE: In real implementations, you will want to use a validation set to check your model's performance while training.\n",
        "\n",
        "## Data PreProcessing\n",
        "Just as in the SimpleNN, we will use `torchvision` to convert these colored images into a tensor. Since we are dealing with colored images, we will also want to standardize these colors (called channels) with a mean of 0.5 and a standard deviation of 0.5. [docs](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP9F9OjuQgUN",
        "outputId": "074a4060-58cf-49f4-dede-c8a9087d95bc"
      },
      "id": "VP9F9OjuQgUN",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bd3f44d4",
      "metadata": {
        "id": "bd3f44d4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# We'll normalize the data to a standard distribution, which helps with training\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Download and load the training data\n",
        "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2805a471",
      "metadata": {
        "id": "2805a471"
      },
      "source": [
        "Now, we can build our CNN model. We will use `nn.Conv2d` for the convolutional layers and `nn.MaxPool2d` for the pooling layers.\n",
        "1. `nn.Conv2d`: This takes three main arguments: `in_channels` (the number of color channels in our image, which is 3 for RGB), `out_channels` (the number of filters we want to apply), and `kernel_size` (the size of our filter, e.g., 5x5). [docs](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
        "\n",
        "2. `nn.MaxPool2d`: This is our pooling layer. The key argument is `kernel_size`, which defines the window size for pooling. A 2x2 kernel with a `stride` of 2 is common, as it halves the dimensions of the feature map. [docs](https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a169a0c4",
      "metadata": {
        "id": "a169a0c4"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F # Can be used to apply activation functions\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 3 input channels (for RGB), 6 output channels, 5x5 kernel\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "\n",
        "        # 2x2 max pooling\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # 6 input channels, 16 output channels, 5x5 kernel\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "\n",
        "        # Fully connected layers for classification\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First Convolutional and Pooling layer\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Second Convolutional and Pooling layer\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Flatten the feature maps for the fully connected layers\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d4f656a",
      "metadata": {
        "id": "1d4f656a"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "306109e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "306109e8",
        "outputId": "5d2f7af7-f87e-4cf3-f005-a243627de272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.6373\n",
            "Epoch [2/10], Loss: 1.3495\n",
            "Epoch [3/10], Loss: 1.2232\n",
            "Epoch [4/10], Loss: 1.1402\n",
            "Epoch [5/10], Loss: 1.0829\n",
            "Epoch [6/10], Loss: 1.0246\n",
            "Epoch [7/10], Loss: 0.9847\n",
            "Epoch [8/10], Loss: 0.9444\n",
            "Epoch [9/10], Loss: 0.9085\n",
            "Epoch [10/10], Loss: 0.8776\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = SimpleCNN()\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "## Training Loop\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Move data to the GPU\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad() # Zero the parameter gradients\n",
        "\n",
        "        outputs = model(images) # Forward pass\n",
        "\n",
        "        loss_value = criterion(outputs, labels) # Compute loss\n",
        "\n",
        "        loss_value.backward() # Backward pass\n",
        "\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "        running_loss += loss_value.item()\n",
        "\n",
        "    print(f'Epoch [{epoch}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0701df5d",
      "metadata": {
        "id": "0701df5d"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ace734f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ace734f6",
        "outputId": "0be59b88-3b81-45ce-93c6-d862b846f869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10,000 test images: 63.35%\n"
          ]
        }
      ],
      "source": [
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "## Evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad(): # Disable gradient calculation\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the network on the 10,000 test images: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f62af5c",
      "metadata": {
        "id": "6f62af5c"
      },
      "source": [
        "This is not that good. Let's assume that the model is underfitting due to its simple nature. We can try to implement a VGG-style network. The core idea behind a VGG-style network is to use a deeper stack of small 3x3 convolutional kernels instead of larger kernels. The network gets its power from its depth rather than its width. \\\n",
        "\n",
        "### Dropout\n",
        "In this new network, we will use the Dropout method between the MLP layers to try and prevent overfitting. [docs](https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "90f0e6fe",
      "metadata": {
        "id": "90f0e6fe"
      },
      "outputs": [],
      "source": [
        "class VGGishCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # VGG-style feature extraction layers\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1: 3 input channels (RGB) -> 64 output channels\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 2: 64 input channels -> 128 output channels\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 3: 128 input channels -> 256 output channels\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        # Classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 4 * 4, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5), # p is a hyperparameter\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "94a565ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94a565ab",
        "outputId": "c1351eb6-6428-4f4d-db54-c7103d33ee67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.7319\n",
            "Epoch [2/10], Loss: 1.2856\n",
            "Epoch [3/10], Loss: 1.0517\n",
            "Epoch [4/10], Loss: 0.9135\n",
            "Epoch [5/10], Loss: 0.8232\n",
            "Epoch [6/10], Loss: 0.7474\n",
            "Epoch [7/10], Loss: 0.6840\n",
            "Epoch [8/10], Loss: 0.6324\n",
            "Epoch [9/10], Loss: 0.6030\n",
            "Epoch [10/10], Loss: 0.5668\n"
          ]
        }
      ],
      "source": [
        "## Training Loop\n",
        "VGG_model = VGGishCNN()\n",
        "VGG_model.to(device)\n",
        "optimizer = optim.Adam(VGG_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Move the data to the GPU\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad() # Zero the parameter gradients\n",
        "\n",
        "        outputs = VGG_model(images) # Forward pass\n",
        "\n",
        "        loss_value = criterion(outputs, labels) # Compute loss\n",
        "\n",
        "        loss_value.backward() # Backward pass\n",
        "\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "        running_loss += loss_value.item()\n",
        "\n",
        "    print(f'Epoch [{epoch}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a44e24f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a44e24f4",
        "outputId": "352dad5a-24ee-4e42-ec4e-d883d9ce74b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 74.71%\n"
          ]
        }
      ],
      "source": [
        "## Evaluation\n",
        "VGG_model.eval() # Set the model to evaluation mode\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad(): # Disable gradient calculation\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = VGG_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the network on the test images: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_correct = 0\n",
        "train_total = 0\n",
        "with torch.no_grad(): # Disable gradient calculation\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = VGG_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "\n",
        "accuracy = 100 * train_correct / train_total\n",
        "print(f'Accuracy of the network on the train images: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "G8GRkSWGTO5U",
        "outputId": "4c77ac02-cda5-4eff-d838-f984cc18466d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "G8GRkSWGTO5U",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the train images: 86.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows us that the model is overfitting. We could use L2 regularization by adding a weight decay to the optimizer. We could also add batch normalization within our layers."
      ],
      "metadata": {
        "id": "JU0zyy8ZTYn9"
      },
      "id": "JU0zyy8ZTYn9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}